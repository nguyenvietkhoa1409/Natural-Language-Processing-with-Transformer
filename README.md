# Natural Language Processing with Transformers 🚀

Welcome to **Natural-Language-Processing-with-Transformer**, a hands-on project by me dedicated to exploring and learning transformer‑based techniques in NLP.

---

## 🌟 About This Project

This repository contains a collection of **Jupyter notebooks**, organized by chapters, that go through real-world NLP tasks using transformer architectures. From translation to named entity recognition (NER), each notebook is my playground to experiment and learn.

Inspired by repositories like *“Natural Language Processing with Transformers”* from the O’Reilly publication, this work strives to bring practical, engaging exercises with Hugging Face models at its core. ([github.com](https://github.com/nlp-with-transformers?utm_source=chatgpt.com))

---

## 📚 What’s Inside

- **Chapter 1:** Introduction to Transformers, tokenization, and basic NLP pipelines  
- **Chapter 2–3:** Language modeling, text classification, and fine‑tuning pretrained models  
- **Chapter 4:** Multilingual NER and handling datasets across languages  
- **Chapter 5:** Hands-on examples including glove embeddings, data preparation (excluding large `.zip` assets)  
- **Chapter 6–7:** Deep dive into advanced tasks like summarization, sequence generation, and neural networks
  
---

## 🎯 Learning Goals

By exploring this repository, i want to:
- Gain a solid understanding of transformer architectures (e.g. BERT, GPT, T5) and their usage for NLP tasks.
- Learn how to fine‑tune pretrained models for classification, generation, and NER.
- Practice data preprocessing, model evaluation, and inference in TensorFlow or PyTorch.
- Organize code and data for clarity and reproducibility.

