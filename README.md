# Natural Language Processing with Transformers ğŸš€

Welcome to **Natural-Language-Processing-with-Transformer**, a hands-on project by me dedicated to exploring and learning transformerâ€‘based techniques in NLP.

---

## ğŸŒŸ About This Project

This repository contains a collection of **Jupyter notebooks**, organized by chapters, that go through real-world NLP tasks using transformer architectures. From translation to named entity recognition (NER), each notebook is my playground to experiment and learn.

Inspired by repositories like *â€œNatural Language Processing with Transformersâ€* from the Oâ€™Reilly publication, this work strives to bring practical, engaging exercises with Hugging Face models at its core. ([github.com](https://github.com/nlp-with-transformers?utm_source=chatgpt.com))

---

## ğŸ“š Whatâ€™s Inside

- **Chapter 1:** Introduction to Transformers, tokenization, and basic NLP pipelines  
- **Chapter 2â€“3:** Language modeling, text classification, and fineâ€‘tuning pretrained models  
- **Chapter 4:** Multilingual NER and handling datasets across languages  
- **Chapter 5:** Hands-on examples including glove embeddings, data preparation (excluding large `.zip` assets)  
- **Chapter 6â€“7:** Deep dive into advanced tasks like summarization, sequence generation, and neural networks
  
---

## ğŸ¯ Learning Goals

By exploring this repository, i want to:
- Gain a solid understanding of transformer architectures (e.g. BERT, GPT, T5) and their usage for NLP tasks.
- Learn how to fineâ€‘tune pretrained models for classification, generation, and NER.
- Practice data preprocessing, model evaluation, and inference in TensorFlow or PyTorch.
- Organize code and data for clarity and reproducibility.

